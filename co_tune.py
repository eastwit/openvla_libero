import torch
import numpy as np
import imageio
import os
from transformers import AutoProcessor, AutoModelForVision2Seq
from huggingface_hub import try_to_load_from_cache
from PIL import Image

# LIBERO ç›¸å…³å¯¼å…¥
from libero.libero import benchmark
from libero.libero.envs import OffScreenRenderEnv
from libero.libero.utils import get_libero_path

# å±è”½ Tokenizer å¹¶è¡Œè­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# è§£å†³ PyTorch 2.6+ å®‰å…¨æ£€æŸ¥é—®é¢˜
try:
    torch.serialization.add_safe_globals([np._core.multiarray._reconstruct])
except AttributeError:
    torch.serialization.add_safe_globals([np.core.multiarray._reconstruct])

# ==========================================
# 1. æ¨¡å‹åŠ è½½å‡½æ•° (4-bit ä¼˜åŒ–)
# ==========================================
def load_vla(img_path, model_id):
    os.environ["HF_ENDPOINT"] = img_path
    filepath = try_to_load_from_cache(model_id, "config.json")
    
    try:
        processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
        vla = AutoModelForVision2Seq.from_pretrained(
            model_id, 
            torch_dtype=torch.float16, 
            low_cpu_mem_usage=True, 
            trust_remote_code=True,
            device_map="auto",
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True
        )
        print("--- OpenVLA (4-bit) åŠ è½½æˆåŠŸï¼ ---")
        return vla, processor
    except Exception as e:
        print(f"åŠ è½½å¤±è´¥: {e}")
        return None, None

# ==========================================
# 2. LIBERO ç¯å¢ƒé…ç½®
# ==========================================
def setup_libero_env(task_suite_name, task_id):
    benchmark_dict = benchmark.get_benchmark_dict()
    task_suite = benchmark_dict[task_suite_name]()
    task = task_suite.get_task(task_id)
    
    task_bddl_file = os.path.join(get_libero_path("bddl_files"), task.problem_folder, task.bddl_file)
    print(f"[ä»»åŠ¡] {task.name} | [æŒ‡ä»¤] {task.language}")

    env_args = {
        "bddl_file_name": task_bddl_file,
        "camera_heights": 256,
        "camera_widths": 256,
        "camera_names": ["agentview"], 
        "reward_shaping": True,
        "control_freq": 20,
    }
    
    env = OffScreenRenderEnv(**env_args)
    env.seed(0)
    obs = env.reset()
    init_states = task_suite.get_task_init_states(task_id)
    env.set_init_state(init_states[0])
    return env, task.language

# ==========================================
# 3. ä¸»ç¨‹åº
# ==========================================
if __name__ == "__main__":
    MODEL_ID = "openvla/openvla-7b"
    HF_MIRROR = "https://hf-mirror.com"
    VIDEO_PATH = "libero_openvla_demo.mp4"
    MAX_STEPS = 2000 
    ACTION_SCALE = 4.0  # å»ºè®®ä» 4.0 å¼€å§‹å°è¯•ï¼Œ10.0 æœ‰ç‚¹å¤ªå¤§äº†

    vla, processor = load_vla(HF_MIRROR, MODEL_ID)
    env, prompt = setup_libero_env("libero_10", 0)
    
    # å¼ºåˆ¶ä½¿ç”¨ ffmpeg å†™å…¥ï¼Œé¿å… Tiff é”™è¯¯
    writer = imageio.get_writer(VIDEO_PATH, fps=20, format='FFMPEG', mode='I')

    obs = env.reset()

    print("ğŸš€ å¯åŠ¨æ§åˆ¶å¾ªç¯...")
    try:
        for step in range(MAX_STEPS):
            # --- ä¿®æ­£è§†è§‰è¾“å…¥ ---
            # é’ˆå¯¹æˆªå›¾ä¸­çš„é¢ å€’é—®é¢˜ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ np.flip è¿›è¡Œæ›´å½»åº•çš„ç¿»è½¬
            raw_image = obs['agentview_image']
            # è¿™ç§ç¿»è½¬æ–¹å¼ç¡®ä¿ç”»é¢åº•åº§åœ¨ä¸‹ï¼Œç‰©ä½“åœ¨ä¸Š
            corrected_image = np.flip(raw_image, axis=0) 
            
            input_pil = Image.fromarray(corrected_image.astype(np.uint8))

            # --- VLA æ¨ç† ---
            with torch.inference_mode():
                inputs = processor(prompt, input_pil).to("cuda", dtype=torch.float16)
                action = vla.predict_action(**inputs, unnorm_key="bridge_orig")

            # --- åŠ¨ä½œç¼©æ”¾ä¸æ‰§è¡Œ ---
            # 10å€å¯èƒ½å¤ªçŒ›ï¼Œè¿™é‡Œç”¨ ACTION_SCALE æ§åˆ¶
            scaled_action = action.astype(np.float64) * ACTION_SCALE
            # å¤¹çˆªåŠ¨ä½œ (æœ€åä¸€ç»´) é€šå¸¸ä¸éœ€è¦ç¼©æ”¾ï¼Œä¿æŒåœ¨åŸèŒƒå›´
            scaled_action[-1] = action[-1] 
            
            obs, reward, done, info = env.step(scaled_action)

            # --- ä¿å­˜è§†é¢‘å¸§ ---
            writer.append_data(corrected_image)

            if step % 10 == 0:
                print(f"Step {step}/{MAX_STEPS} | åŠ¨ä½œæ‰§è¡Œä¸­...")
            
            if step % 5 == 0:
                torch.cuda.empty_cache()

            if done:
                print("ğŸ ä»»åŠ¡å®Œæˆï¼")
                break

    except Exception as e:
        print(f"è¿è¡Œæ—¶é”™è¯¯: {e}")
    finally:
        writer.close()
        env.close()
        print(f"âœ¨ è§†é¢‘å·²ä¿å­˜è‡³: {os.path.abspath(VIDEO_PATH)}")