import os
# ç¯å¢ƒä¸é•œåƒè®¾ç½®
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import torch
import numpy as np
import imageio
from transformers import AutoProcessor, AutoModelForVision2Seq
from PIL import Image
import sys

# ==========================================
# å·¥å…·å‡½æ•°ï¼šå¯¹é½å¾®è°ƒæ—¶çš„ 90% Center Crop
# ==========================================
def get_openvla_input(raw_image):
    """
    1. ä¿®æ­£ç¿»è½¬
    2. å–ä¸­å¿ƒ 90% åŒºåŸŸ (è¿™æ˜¯å¾®è°ƒæ—¶çš„è§„èŒƒ)
    """
    # ä¿®æ­£ LIBERO æ¸²æŸ“é¢ å€’
    corrected = np.flip(raw_image, axis=0)
    img = Image.fromarray(corrected.astype(np.uint8))
    
    width, height = img.size
    # è®¡ç®— 90% é¢ç§¯å¯¹åº”çš„è¾¹é•¿æ¯”ä¾‹ (çº¦ 0.9487)
    scale = 0.9487 
    new_w, new_h = int(width * scale), int(height * scale)
    
    left = (width - new_w) / 2
    top = (height - new_h) / 2
    right = (width + new_w) / 2
    bottom = (height + new_h) / 2
    
    # è£å‰ªå¹¶ Resize åˆ°æ¨¡å‹æ ‡å‡†çš„ 224
    input_pil = img.crop((left, top, right, bottom)).resize((224, 224), Image.LANCZOS)
    return input_pil, corrected

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
if __name__ == "__main__":
    MODEL_ID = "openvla/openvla-7b-finetuned-libero-spatial"
    VIDEO_PATH = "libero_spatial_optimized.mp4"
    
    # 1. åŠ è½½æ¨¡å‹ (å¯¹é½ 4-bit å’Œç‰¹å®šç»Ÿè®¡é‡)
    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)
    vla = AutoModelForVision2Seq.from_pretrained(
        MODEL_ID, 
        torch_dtype=torch.float16, 
        device_map="auto",
        load_in_4bit=True, 
        trust_remote_code=True
    )

    # 2. ç¯å¢ƒåˆå§‹åŒ– (LIBERO-Spatial)
    from libero.libero import benchmark
    from libero.libero.envs import OffScreenRenderEnv
    from libero.libero.utils import get_libero_path

    benchmark_dict = benchmark.get_benchmark_dict()
    task_suite = benchmark_dict["libero_spatial"]()
    TASK_ID = 1 # ä½ å¯ä»¥æ›´æ¢ ID
    task = task_suite.get_task(TASK_ID)
    task_bddl = os.path.join(get_libero_path("bddl_files"), task.problem_folder, task.bddl_file)

    env = OffScreenRenderEnv(
        bddl_file_name=task_bddl,
        camera_heights=256,
        camera_widths=256,
        camera_names=["agentview"],
        control_freq=20,
    )
    
    obs = env.reset()
    env.set_init_state(task_suite.get_task_init_states(TASK_ID)[0])

    # 3. æ¨ç†å¾ªç¯
    prompt = f"In: What action should the robot take to {task.language}?\nOut:"
    writer = imageio.get_writer(VIDEO_PATH, fps=20, format='FFMPEG', mode='I')

    print(f"ğŸš€ æ­£åœ¨æ‰§è¡Œå¯¹é½åçš„ä»»åŠ¡: {task.language}")

    try:
        for step in range(600):
            # è·å– 90% Crop åçš„è¾“å…¥
            input_pil, render_frame = get_openvla_input(obs['agentview_image'])
            
            with torch.inference_mode():
                prompt="In: What action should the robot take to {open the draw}?\nOut:"
                inputs = processor(prompt, input_pil).to("cuda", dtype=torch.float16)
                # ã€å…³é”®ã€‘ä½¿ç”¨ç¬”è®°ä¸­ç¡®å®šçš„ libero_spatial ç»Ÿè®¡é‡
                action = vla.predict_action(**inputs, unnorm_key="libero_spatial")

            # åŠ¨ä½œæ‰§è¡Œ
            scaled_action = action.astype(np.float64)
            # å¤¹çˆªé€»è¾‘å¯¹é½
            scaled_action[-1] = 1.0 if action[-1] > 0.5 else -1.0
            
            obs, reward, done, info = env.step(scaled_action)
            writer.append_data(render_frame)

            if step % 20 == 0: print(f"Step {step}...")
            if done: break

    finally:
        writer.close()
        env.close()
        print(f"âœ¨ å½•åˆ¶å®Œæˆ: {VIDEO_PATH}")